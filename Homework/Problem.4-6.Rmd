---
title: "STATS 202 Homework 5"
author: Peter Lee
output: html_document
---

## Homework 5

### 4. ISL, Chapter 6, Exercise 8
In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

#### A. Use the rnorm() function to generate a predictor X of length n=100, as well as a noise vector E of length n = 100.
```{r}
set.seed(12)
X = rnorm(100)
err = rnorm(100)
```

#### B. Generate a response vector Y of length n=100 according to model...
```{r}
b0 = 4
b1 = 1
b2 = 2
b3 = 3
Y = b0 + b1*X + b2*X^2 + b3*X^3 + err
```

#### C. Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X, X^2,...,X^10. What is the best model obtained according to C_p, BIC, and adjusted R^2? Show some plots to provide evidence for your answer and report the coefficients of the best model obtained. Note that you will need to use the data.frame() function to create a single data set containing both X and Y.
```{r}
library(leaps)
data.xy = data.frame(x=X,y=Y)
regfit.best.4c = regsubsets(y ~ poly(x,10,raw=TRUE), data = data.xy, nvmax = 10)
reg.summary.4c = summary(regfit.best.4c)
reg.summary.4c

which.max(reg.summary.4c$adjr2)
plot(reg.summary.4c$adjr2, xlab = "No. of Vars.", ylab = "Adj. R^2", type = "l")
points(3,reg.summary.4c$adjr2[3], col = "red")

which.min(reg.summary.4c$cp)
plot(reg.summary.4c$cp, xlab = "No. of Vars.", ylab = "C_p Error", type = "l")
points(3,reg.summary.4c$cp[3], col = "red")

which.min(reg.summary.4c$bic)
plot(reg.summary.4c$bic, xlab = "No. of Vars.", ylab = "BIC Error", type = "l")
points(3,reg.summary.4c$bic[3], col = "red")

coefficients(regfit.best.4c,3)


```

#### D. Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?
```{r}
regfit.for.4d = regsubsets(y ~ poly(x,10,raw=TRUE), data = data.xy, nvmax = 10, method = "forward")
reg.summary.for.4d = summary(regfit.for.4d)
reg.summary.for.4d

which.max(reg.summary.for.4d$adjr2)
plot(reg.summary.for.4d$adjr2, xlab = "No. of Vars.", ylab = "Adj. R^2", type = "l")
points(3,reg.summary.for.4d$adjr2[3], col = "red")

which.min(reg.summary.for.4d$cp)
plot(reg.summary.for.4d$cp, xlab = "No. of Vars.", ylab = "C_p Error", type = "l")
points(3,reg.summary.for.4d$cp[3], col = "red")

which.min(reg.summary.for.4d$bic)
plot(reg.summary.for.4d$bic, xlab = "No. of Vars.", ylab = "BIC Error", type = "l")
points(3,reg.summary.for.4d$bic[3], col = "red")


regfit.back.4d = regsubsets(y ~ poly(x,10,raw=TRUE), data = data.xy, nvmax = 10, method = "backward")
reg.summary.back.4d = summary(regfit.back.4d)
reg.summary.back.4d

which.max(reg.summary.back.4d$adjr2)
plot(reg.summary.back.4d$adjr2, xlab = "No. of Vars.", ylab = "Adj. R^2", type = "l")
points(3,reg.summary.back.4d$adjr2[3], col = "red")

which.min(reg.summary.back.4d$cp)
plot(reg.summary.back.4d$cp, xlab = "No. of Vars.", ylab = "C_p Error", type = "l")
points(3,reg.summary.back.4d$cp[3], col = "red")

which.min(reg.summary.back.4d$bic)
plot(reg.summary.back.4d$bic, xlab = "No. of Vars.", ylab = "BIC Error", type = "l")
points(3,reg.summary.back.4d$bic[3], col = "red")

coefficients(regfit.for.4d,3)
coefficients(regfit.back.4d,3)

```
My answers stay the same as part (c). This could possibly be due to the simple nature of the problem.


#### E. Now fit a lasso model to the simulated data, again using X, X^2,..., X^10 as predictors. Use cross-validation to select the optimal value of lambda. Create plots of the cross-validation error as a function of lambda. Report the resulting coefficient estimates, and discuss the results obtained.
```{r}
library(glmnet)
mat.x.4e = model.matrix(y ~ poly(x,10,raw=TRUE), data = data.xy)[,-1]

lasso.mod = cv.glmnet(mat.x.4e, Y, alpha=1)
summary(lasso.mod)
lambda.min = lasso.mod$lambda.min
lambda.min

plot(lasso.mod)

best.model = glmnet(mat.x.4e, Y, alpha = 1)
lasso.pred = predict(best.model, s=lambda.min, type = "coefficients")
lasso.pred
```
There are 4 variables remaining in lasso, which are B^1 through B^4. The coefficients get progressively smaller, which makes sense as the true function of Y only goes until the X^3. B^4 is very small.



#### F. Now generate a response vector Y according to the model... and perform best subset selection and the lasso. Discuss the results obtained.
```{r}
b7 = 5
Y = b0 + b7*X^7 + err
data.xy.4f = data.frame(y = Y, x = X)
```

best subset selection
```{r}
regfit.best.4f = regsubsets(y ~ poly(x,10,raw=TRUE), data = data.xy.4f, nvmax = 10)
reg.summary.4f = summary(regfit.best.4f)
reg.summary.4f

which.max(reg.summary.4f$adjr2)
plot(reg.summary.4f$adjr2, xlab = "No. of Vars.", ylab = "Adj. R^2", type = "l")
points(3,reg.summary.4f$adjr2[3], col = "red")

which.min(reg.summary.4f$cp)
plot(reg.summary.4f$cp, xlab = "No. of Vars.", ylab = "C_p Error", type = "l")
points(1,reg.summary.4f$cp[1], col = "red")

which.min(reg.summary.4f$bic)
plot(reg.summary.4f$bic, xlab = "No. of Vars.", ylab = "BIC Error", type = "l")
points(1,reg.summary.4f$bic[1], col = "red")

coef(regfit.best.4f, id = 1)
coef(regfit.best.4f, id = 3)
```

lasso
```{r}
mat.x.4f = model.matrix(y ~ poly(x, 10, raw = T), data = data.xy.4f)[, -1]
mod.lasso.4f = cv.glmnet(mat.x.4f, Y, alpha = 1)
lambda.min.4f = mod.lasso.4f$lambda.min
lambda.min.4f

best.model.4f = glmnet(mat.x.4f, Y, alpha = 1)
lasso.pred.4f = predict(best.model.4f, s = lambda.min.4f, type = "coefficients")
lasso.pred.4f
```
In best subset selection, C_p and BIC do a good job of picking one variable, as Adj. R^2 picks 3.
The lasso picks 2 variables, although the one that doesn't matter (X^5) has a small coefficient.




### 5. ISL, Chapter 6, Exercise 9
In this exercise, we will predict the number of applications received using the other variables in the College data set.

#### A. Split the data set into a training set and a test set.
```{r}
library(ISLR)
set.seed(10)
sum(is.na(College))

train = sample(1:dim(College)[1], dim(College)[1]/2)
test = -train
College.train = College[train, ]
College.test = College[test, ]
```

#### B. Fit a linear model using least squares on the training set, and report the test error obtained.
```{r}
lm.fit.5b = lm(Apps ~ ., data=College.train)
lm.pred.5b = predict(lm.fit.5b, College.test)
mean((College.test[, "Apps"] - lm.pred.5b)^2)
```
Test RSS is 1,016,996.

#### C. Fit a ridge regression model on the training set, with lambda chosen by cross-validation. Report the test error obtained.
```{r}
library(glmnet)
train.mat.5c = model.matrix(Apps ~ ., data=College.train)
test.mat.5c = model.matrix(Apps ~ ., data=College.test)
grid = 10 ^ seq(4, -2, length=100)
ridge.mod.5c = cv.glmnet(train.mat.5c, College.train[, "Apps"], alpha=0, lambda=grid)
lambda.best.5c = ridge.mod.5c$lambda.min
lambda.best.5c

ridge.pred.5c = predict(ridge.mod.5c, s=lambda.best.5c, newx = test.mat.5c)
mean((College.test[, "Apps"] - ridge.pred.5c)^2)

```
Test RSS is very slightly lower than OLS' Test RSS, at 1,016,838.


#### D. Fit a lasso model on the training set, with lambda chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.
```{r}
lasso.mod.5d = cv.glmnet(train.mat.5c, College.train[, "Apps"], alpha=1, lambda=grid)
lambda.best.5d = lasso.mod.5d$lambda.min
lambda.best.5d

lasso.pred.5d = predict(lasso.mod.5d, s = lambda.best.5d, newx = test.mat.5c)
mean((College.test[, "Apps"] - lasso.pred.5d)^2)
```
The test RSS is lower at 928,205. Coefficients look like below:

```{r}
lasso.mod.5d2 = glmnet(model.matrix(Apps~., data=College), College[, "Apps"], alpha=1)
predict(lasso.mod.5d2, s=lambda.best.5d, type="coefficients")
```

#### E. Fit a PCR model on the training set, with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation.
```{r}
library(pls)
pcr.fit.5e = pcr(Apps ~ ., data=College.train, scale=TRUE, validation="CV")
validationplot(pcr.fit.5e, val.type="MSEP")

pcr.pred.5e = predict(pcr.fit.5e, College.test, ncomp=17)
mean((College.test[, "Apps"] - data.frame(pcr.pred.5e))^2)
```
Test error is 1,371,446, and M is 17, which is least squares.


#### F. Fit a PLS model on the training set, with M chosen by cross-validation. Report the test error obtained, along with the value of M selected by cross-validation.
```{r}
pls.fit.5f = plsr(Apps ~ ., data=College.train, scale=TRUE, validation="CV")
validationplot(pls.fit.5f, val.type="MSEP")

pls.pred.5f = predict(pls.fit.5f, College.test, ncomp=6)
mean((College.test[, "Apps"] - data.frame(pls.pred.5f))^2)
```
Test error is 1,009,150, and the M chosen is 6, as after 6, it doesn't matter how many components are chosen. So using the 1 SE rule, I got 6.


#### G. Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?
The linear model, the ridge regression, and PLS model have very similar test RSS's. The Lasso model's error is low, while that of the PCR is high. By looking at the summaries, we can see that the R^2 values are 0.93 for the linear model, which shows that the variables would indeed do a good job of predicting the number of college apps. received.





### 6. ISL, Chapter 6, Exercise 10

#### A. Generate a dataset with p=20 features, n=1000 observations, and an associated quantitative response vector generated according to the model...
```{r}
set.seed(1)
n = 1000
p = 20

beta = rnorm(p+1) # add one for the constant beta_0 
zeros = c(1,3,5,16,18)
beta[zeros] = 0
err = rnorm(p)

X = c(rep(1,n), rnorm(n*p))
X = matrix(X, nrow=n, ncol=(p+1), byrow=FALSE)

Y = X %*% beta + err
```


#### B. Split your data set into a training set containing 100 observations and a test set containing 900 observations.
```{r}
dataframe = data.frame(Y, X[,-1])

train.set = sample(1:n, 100)
test.set = (1:n)[-train.set]
```

#### C. Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.
```{r}
library(leaps)
regfit.full = regsubsets(Y ~ ., data = dataframe[train.set,], nvmax=p)
reg.summary = summary(regfit.full)
reg.summary

training.mat = model.matrix( Y ~ ., data = dataframe[train.set,] )
val.errors = rep(NA,p)
for(i in 1:p){
  coefi = coef(regfit.full, id=i)
  pred = training.mat[,names(coefi)] %*% coefi
  val.errors[i] = mean((dataframe$Y[train.set] - pred)^2) 
}

plot( 1:20, val.errors, xlab='number of predictors', ylab='training MSE', type='o', col='red', ylim=c(0,9) )
```

#### D. Plot the test set MSE associated with the best model of each size.
```{r}
test.mat = model.matrix(Y ~ ., data = dataframe[test.set,])
val.errors.d = rep(NA,p)
for(i in 1:p){
  coefi = coef(regfit.full, id=i)
  pred = test.mat[,names(coefi)] %*% coefi
  val.errors.d[i] = mean((dataframe$Y[test.set] - pred)^2) 
}
```

#### E. For which model size does the test MSE take on its minimum value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimized for an intermediate model size.
```{r}
which.min(val.errors.d) 
```
Test MSE is minimized at size 15.


#### F. How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values.
```{r}
coef(regfit.full, id = 15)
```
Coefficient X3 is correctly zeroed out, but X1,X5,X16,X18 weren't zeroed out. X1, X16, and X18 are closer to 0, but X5 is not.

#### G. Create a plot displaying... for a range of values of r, where B^r_j is the jth coefficient estimate for the best model containing r coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)?
```{r}
val.errors.g = rep(NA, p)
x.cols = colnames(X, do.NULL = FALSE, prefix = "x.")
a = rep(NA, p)
b = rep(NA, p)
for (i in 1:p){
    coefi = coef(regfit.full, id = i)
    a[i] = length(coefi) - 1
    b[i] = sqrt(sum((beta[x.cols %in% names(coefi)] - coefi[names(coefi) %in% x.cols])^2))
}
plot(x = a, y = b, xlab = "No. of Coefficients", ylab = "Error Between Estimated/Straight Coefficients")
```
I observed a straight line, this is different from the non-linear line observed in part d.


